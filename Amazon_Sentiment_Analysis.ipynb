{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TeabPAKD-kZh"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading games_reviews.json file using the json library\n",
        "with open('games_reviews.json', 'r') as handle:\n",
        "    data = [json.loads(line) for line in handle]\n",
        " \n",
        "# creating lists for sentences,labels and urls\n",
        "sentences = [] # headlines\n",
        "labels = [] # labels\n",
        " \n",
        "# iterating through the json data and loding \n",
        "# the requisite values into our python lists\n",
        "for item in data:\n",
        "    sentences.append(item['reviewText'])\n",
        "    if item[\"overall\"] > 3:\n",
        "      labels.append(1)\n",
        "    else:\n",
        "      labels.append(0)"
      ],
      "metadata": {
        "id": "QFUqRqUF-sh9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_size = 134078\n",
        "\n",
        "training_sentences = sentences[0:training_size]\n",
        "testing_sentences = sentences[training_size:]\n",
        "\n",
        "training_labels = labels [0:training_size]\n",
        "testing_labels = labels [training_size:]"
      ],
      "metadata": {
        "id": "qrH4V0-q_3eu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab_size = 10000\n",
        "max_length = 100\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "# fitting tokenizer only to training set\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# creating training sequences and padding them\n",
        "traning_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(traning_sequences,maxlen = max_length,\n",
        "                                padding = padding_type,\n",
        "                                truncating=trunc_type,\n",
        "                                )\n",
        "\n",
        "# creating  testing sequences and padding them using same tokenizer\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences,maxlen = max_length,\n",
        "                                padding = padding_type,\n",
        "                                truncating=trunc_type,\n",
        "                                )"
      ],
      "metadata": {
        "id": "720UuzDy__5U"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# converting all variables to numpy arrays, to be able to work with tf version 2\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ],
      "metadata": {
        "id": "uFCVhoLSAH1E"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_dim = 16\n",
        "\n",
        "# creating a model for sentiment analysis\n",
        "model  = tf.keras.Sequential([\n",
        "                # addinging an Embedding layer for Neural Network to learn the vectors\n",
        "                tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
        "                # Global Average pooling is similar to adding up vectors in this case\n",
        "                tf.keras.layers.GlobalAveragePooling1D(),\n",
        "                tf.keras.layers.Dense(24, activation = 'relu'),\n",
        "                tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "SmP64YcxAIJs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "history = model.fit(training_padded,training_labels, epochs = num_epochs,\n",
        "                    validation_data = (testing_padded,testing_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8A5vywBALwp",
        "outputId": "11fc4054-6fe9-4c6f-f207-92905d6a6935"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4190/4190 [==============================] - 26s 6ms/step - loss: 0.4375 - accuracy: 0.8015 - val_loss: 0.3880 - val_accuracy: 0.8361\n",
            "Epoch 2/10\n",
            "4190/4190 [==============================] - 27s 6ms/step - loss: 0.3724 - accuracy: 0.8346 - val_loss: 0.3669 - val_accuracy: 0.8394\n",
            "Epoch 3/10\n",
            "4190/4190 [==============================] - 24s 6ms/step - loss: 0.3588 - accuracy: 0.8412 - val_loss: 0.3695 - val_accuracy: 0.8394\n",
            "Epoch 4/10\n",
            "4190/4190 [==============================] - 25s 6ms/step - loss: 0.3502 - accuracy: 0.8451 - val_loss: 0.3753 - val_accuracy: 0.8393\n",
            "Epoch 5/10\n",
            "4190/4190 [==============================] - 27s 6ms/step - loss: 0.3421 - accuracy: 0.8486 - val_loss: 0.3746 - val_accuracy: 0.8391\n",
            "Epoch 6/10\n",
            "4190/4190 [==============================] - 24s 6ms/step - loss: 0.3351 - accuracy: 0.8523 - val_loss: 0.3806 - val_accuracy: 0.8376\n",
            "Epoch 7/10\n",
            "4190/4190 [==============================] - 24s 6ms/step - loss: 0.3282 - accuracy: 0.8555 - val_loss: 0.3892 - val_accuracy: 0.8340\n",
            "Epoch 8/10\n",
            "4190/4190 [==============================] - 24s 6ms/step - loss: 0.3227 - accuracy: 0.8578 - val_loss: 0.3954 - val_accuracy: 0.8330\n",
            "Epoch 9/10\n",
            "4190/4190 [==============================] - 25s 6ms/step - loss: 0.3179 - accuracy: 0.8599 - val_loss: 0.4034 - val_accuracy: 0.8334\n",
            "Epoch 10/10\n",
            "4190/4190 [==============================] - 24s 6ms/step - loss: 0.3138 - accuracy: 0.8616 - val_loss: 0.4051 - val_accuracy: 0.8327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# forming new sentences for testing, feel free to experiment\n",
        "# sentence 1 is bit sarcastic, whereas sentence two is a general statment.\n",
        "new_sentence = [\n",
        "                \"This game is just filled with bugs. Completely garbage\",\n",
        "                \"This might be GOTY. Most fun had with my family.\"]\n",
        "\n",
        "# Converting the sentences to sequences using tokenizer\n",
        "new_sequences = tokenizer.texts_to_sequences(new_sentence)\n",
        "# padding the new sequences to make them have same dimensions\n",
        "new_padded = pad_sequences(new_sequences, maxlen = max_length,\n",
        "                           padding = padding_type,\n",
        "                           truncating = trunc_type)\n",
        "\n",
        "new_padded = np.array(new_padded )\n",
        "\n",
        "print(model.predict(new_padded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr5UxniAD74-",
        "outputId": "2793fae5-a768-45c0-9a00-dcd24b7ffdff"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 115ms/step\n",
            "[[0.11798449]\n",
            " [0.9214925 ]]\n"
          ]
        }
      ]
    }
  ]
}