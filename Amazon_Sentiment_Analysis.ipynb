{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TeabPAKD-kZh"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QFUqRqUF-sh9"
      },
      "outputs": [],
      "source": [
        "# loading games_reviews.json file using the json library\n",
        "with open('games_reviews.json', 'r') as handle:\n",
        "    data = [json.loads(line) for line in handle]\n",
        " \n",
        "# creating lists for reviews and labels (whether they're positive or not)\n",
        "reviews = [] # reviews\n",
        "labels = [] # labels\n",
        " \n",
        "# iterating through the json data and loading \n",
        "# the requisite values into python lists\n",
        "for item in data:\n",
        "    reviews.append(item['reviewText'])\n",
        "    # labelling reviews that give more than 3 stars positive (1)\n",
        "    if item[\"overall\"] > 3:\n",
        "      labels.append(1)\n",
        "    else:\n",
        "      # labelling reviews that give less than or equal to 3 stars negative (0)\n",
        "      labels.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qrH4V0-q_3eu"
      },
      "outputs": [],
      "source": [
        "# training size is 80% of data (231780)\n",
        "training_size = 185424\n",
        "\n",
        "training_reviews = reviews[0:training_size]\n",
        "testing_reviews = reviews[training_size:]\n",
        "\n",
        "training_labels = labels [0:training_size]\n",
        "testing_labels = labels [training_size:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "720UuzDy__5U"
      },
      "outputs": [],
      "source": [
        "\n",
        "vocab_size = 15000\n",
        "max_length = 100\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "# fitting tokenizer only to training set\n",
        "tokenizer.fit_on_texts(training_reviews)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# creating training sequences and padding them\n",
        "traning_sequences = tokenizer.texts_to_sequences(training_reviews)\n",
        "training_padded = pad_sequences(traning_sequences,maxlen = max_length,\n",
        "                                padding = padding_type,\n",
        "                                truncating=trunc_type,\n",
        "                                )\n",
        "\n",
        "# creating  testing sequences and padding them using same tokenizer\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_reviews)\n",
        "testing_padded = pad_sequences(testing_sequences,maxlen = max_length,\n",
        "                                padding = padding_type,\n",
        "                                truncating=trunc_type,\n",
        "                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uFCVhoLSAH1E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# converting all variables to numpy arrays, to be able to work with tf version 2\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SmP64YcxAIJs"
      },
      "outputs": [],
      "source": [
        "\n",
        "embedding_dim = 16\n",
        "\n",
        "# creating a model for sentiment analysis\n",
        "model  = tf.keras.Sequential([\n",
        "                # addinging an Embedding layer for Neural Network to learn the vectors\n",
        "                tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
        "                # Global Average pooling is similar to adding up vectors in this case\n",
        "                tf.keras.layers.GlobalAveragePooling1D(),\n",
        "                tf.keras.layers.Dense(24, activation = 'relu'),\n",
        "                tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8A5vywBALwp",
        "outputId": "54d0ecd2-69c5-4178-b908-a0bf0b52f647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5795/5795 [==============================] - 48s 8ms/step - loss: 0.4194 - accuracy: 0.8106 - val_loss: 0.3499 - val_accuracy: 0.8484\n",
            "Epoch 2/10\n",
            "5795/5795 [==============================] - 47s 8ms/step - loss: 0.3615 - accuracy: 0.8415 - val_loss: 0.3519 - val_accuracy: 0.8486\n",
            "Epoch 3/10\n",
            "5795/5795 [==============================] - 49s 8ms/step - loss: 0.3477 - accuracy: 0.8474 - val_loss: 0.3497 - val_accuracy: 0.8491\n",
            "Epoch 4/10\n",
            "5795/5795 [==============================] - 49s 8ms/step - loss: 0.3378 - accuracy: 0.8511 - val_loss: 0.3633 - val_accuracy: 0.8440\n",
            "Epoch 5/10\n",
            "5795/5795 [==============================] - 51s 9ms/step - loss: 0.3288 - accuracy: 0.8553 - val_loss: 0.3628 - val_accuracy: 0.8463\n",
            "Epoch 6/10\n",
            "5795/5795 [==============================] - 50s 9ms/step - loss: 0.3206 - accuracy: 0.8598 - val_loss: 0.3677 - val_accuracy: 0.8432\n",
            "Epoch 7/10\n",
            "5795/5795 [==============================] - 51s 9ms/step - loss: 0.3130 - accuracy: 0.8647 - val_loss: 0.3770 - val_accuracy: 0.8435\n",
            "Epoch 8/10\n",
            "5795/5795 [==============================] - 50s 9ms/step - loss: 0.3053 - accuracy: 0.8688 - val_loss: 0.3810 - val_accuracy: 0.8411\n",
            "Epoch 9/10\n",
            "5795/5795 [==============================] - 47s 8ms/step - loss: 0.2974 - accuracy: 0.8735 - val_loss: 0.3870 - val_accuracy: 0.8379\n",
            "Epoch 10/10\n",
            "5795/5795 [==============================] - 52s 9ms/step - loss: 0.2897 - accuracy: 0.8775 - val_loss: 0.4014 - val_accuracy: 0.8383\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "history = model.fit(training_padded,training_labels, epochs = num_epochs,\n",
        "                    validation_data = (testing_padded,testing_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# forming new reviews for testing\n",
        "# review 1 is very negative, whereas review two is a very positive statment.\n",
        "# review 3 is slightly negative\n",
        "new_reviews = [\n",
        "                \"This game is just filled with bugs. Completely garbage\",\n",
        "                \"This might be GOTY. Most fun I had with my family.\",\n",
        "                \"This game is alright. Nothing special and very grindy\"]\n",
        "\n",
        "# Converting the reviews to sequences using tokenizer\n",
        "new_sequences = tokenizer.texts_to_sequences(new_reviews)\n",
        "# padding the new sequences to make them have same dimensions\n",
        "new_padded = pad_sequences(new_sequences, maxlen = max_length,\n",
        "                           padding = padding_type,\n",
        "                           truncating = trunc_type)\n",
        "\n",
        "new_padded = np.array(new_padded )\n",
        "\n",
        "print(model.predict(new_padded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n2Uw9roDJjd",
        "outputId": "3167495c-9e4e-4426-e7ac-108d6499a160"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "[[0.0939511 ]\n",
            " [0.92526984]\n",
            " [0.22808737]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}